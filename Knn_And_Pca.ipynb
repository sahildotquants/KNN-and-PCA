{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN and PCA Assignment**"
      ],
      "metadata": {
        "id": "IyNl8QwxgI1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?**\n"
      ],
      "metadata": {
        "id": "QD4FrgpXgMg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**-Nearest Neighbors (KNN) is a supervised learning algorithm used for both classification and regression problems. It’s one of the simplest and most intuitive machine learning methods — it makes predictions based on the idea that similar data points tend to have similar outputs.\n",
        "\n",
        "**How it works for Classification ?**\n",
        "\n",
        "Predict whether a fruit is apple or orange based on weight and color:\n",
        "\n",
        "KNN finds the 3 nearest fruits in the training data.\n",
        "\n",
        "If 2 are apples and 1 is orange → predict apple.\n",
        "\n",
        "**How it works for Regression ?**\n",
        "\n",
        "Predict a person’s income based on age and education:\n",
        "\n",
        "KNN finds the 3 nearest people in the training set.\n",
        "\n",
        "Takes the average income of those 3 → predicted income."
      ],
      "metadata": {
        "id": "2tgLuWpIhMSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?**\n"
      ],
      "metadata": {
        "id": "C6MKoOF1h0fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:-** The Curse of Dimensionality refers to the problems that arise when the number of features (dimensions) in your dataset becomes very large.\n",
        "As dimensions increase, the data becomes sparse, and the concept of “closeness” or “distance” between points becomes less meaningful.\n",
        "\n",
        "**Effect on KNN Performance**\n",
        "\n",
        "KNN relies on distance to find the nearest neighbors.\n",
        "When the number of features increases:\n",
        "\n",
        "**All distances become similar** — the difference between the nearest and farthest neighbors becomes very small.\n",
        "\n",
        "**Distance loses meaning** — the algorithm struggles to identify which points are truly “close”.\n",
        "\n",
        "**Increased noise** — irrelevant or redundant features distort distance calculations.\n",
        "\n",
        "**Model accuracy drops** — predictions become unreliable because neighbors may not actually be similar."
      ],
      "metadata": {
        "id": "-M0XViZ5h-xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**"
      ],
      "metadata": {
        "id": "uNA1cYQhicMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:-** **Principal Component Analysis (PCA)** is a dimensionality reduction technique used to transform a large set of features into a smaller one while preserving as much variance (information) as possible.\n",
        "\n",
        "It does this by creating new features (called principal components) that are:\n",
        "\n",
        "Linear combinations of the original features.\n",
        "\n",
        "Uncorrelated with each other.\n",
        "\n",
        "Ordered by importance — the first few components capture most of the data’s variability\n",
        "\n",
        "**How it is different from Feature Selection ?**\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction method that creates new features from the old ones.\n",
        "It doesn’t remove features — it transforms them. These new features (called principal components) are combinations of the original variables and are designed to capture the maximum amount of information (variance) in the data.\n",
        "\n",
        "Think of PCA as taking many correlated features and compressing them into a smaller number of uncorrelated ones. For example, instead of 10 related financial indicators, PCA might give you 2–3 new components that represent most of the variation those 10 had together.\n",
        "\n",
        "However, these new features are not interpretable — you can’t easily say what each principal component “means” in real-world terms because it’s a mathematical mix of many features."
      ],
      "metadata": {
        "id": "h66IQZiPir-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?**"
      ],
      "metadata": {
        "id": "BNUYf0zRjkRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Principal Component Analysis (PCA)**, eigenvalues and eigenvectors come from the covariance matrix of your data.\n",
        "They are the mathematical foundation that allows PCA to identify the directions (axes) where the data varies the most.\n",
        "\n",
        "**Eigenvectors** represent the directions (axes) of maximum variance in the data —\n",
        "these are your principal components.\n",
        "Each eigenvector shows how much each original feature contributes to a principal component.\n",
        "\n",
        "**Eigenvalues** represent the amount of variance captured by each eigenvector —\n",
        "they tell you how important each principal component is.\n",
        "A higher eigenvalue means that component captures more information (variance) from the data\n",
        "\n",
        "**Why They’re Important in PCA ?**\n",
        "\n",
        "***Identify Principal Components:*** Eigenvectors define the directions (axes) for the new reduced feature space.\n",
        "\n",
        "***Measure Information Content:*** Eigenvalues tell you how much information (variance) each component carries.\n",
        "\n",
        "***Dimensionality Reduction:*** By keeping only the components with the largest eigenvalues, PCA reduces data size while retaining most information.\n",
        "\n",
        "***Noise Filtering***: Components with very small eigenvalues usually represent noise, which can be discarded."
      ],
      "metadata": {
        "id": "3ksQSpt6jwaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?**\n"
      ],
      "metadata": {
        "id": "L0qiAejykZCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer-** **bold text**KNN (K-Nearest Neighbors)** is a distance-based algorithm, and **PCA (Principal Component Analysis**) is a dimensionality reduction technique.\n",
        "When used together in a single pipeline, PCA helps KNN perform better — especially when the dataset has many features or correlated variables.\n",
        "\n",
        "**How They Work Together ?**\n",
        "\n",
        "**PCA Step (Preprocessing)**:\n",
        "\n",
        "PCA reduces the number of features by creating new, uncorrelated components that capture most of the data’s variance.\n",
        "\n",
        "It removes noise and redundant information.\n",
        "\n",
        "This makes the data simpler and cleaner for the next step.\n",
        "\n",
        "**KNN Step (Modeling)**:\n",
        "\n",
        "KNN then works on this transformed, lower-dimensional data.\n",
        "\n",
        "Since KNN depends on distance (Euclidean or similar), having fewer, uncorrelated features makes distances more meaningful and less distorted."
      ],
      "metadata": {
        "id": "wYC-IGxzmqjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.**\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n"
      ],
      "metadata": {
        "id": "DF6DNP8enFE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# Initialize KNN\n",
        "knn_raw = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train\n",
        "knn_raw.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_raw = knn_raw.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Accuracy without scaling:\", round(acc_raw, 3))\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize KNN again\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train on scaled data\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy with scaling:\", round(acc_scaled, 3))\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Without Scaling: {acc_raw:.3f}\")\n",
        "print(f\"With Scaling   : {acc_scaled:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgzonlI8oZdw",
        "outputId": "c41de229-33a2-4f1b-ecd3-2e7260bbd302"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.722\n",
            "Accuracy with scaling: 0.944\n",
            "\n",
            "Comparison:\n",
            "Without Scaling: 0.722\n",
            "With Scaling   : 0.944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**"
      ],
      "metadata": {
        "id": "hMVrbzAro_bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 3: Standardize the Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 4: Apply PCA\n",
        "pca = PCA()   # keep all components\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 5: Print Explained Variance Ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
        "\n",
        "# Optional: print total variance retained\n",
        "print(\"\\nTotal variance retained:\", round(sum(pca.explained_variance_ratio_), 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5_aULaupQ6Z",
        "outputId": "2f15e427-30b1-4cdf-8c0d-d784e475cfb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Total variance retained: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.**\n"
      ],
      "metadata": {
        "id": "s4_KecS_pZ6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Step 3: Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 4: Standardize Data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train KNN on Original Scaled Data\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "print(\"Accuracy on original scaled data:\", round(acc_original, 3))\n",
        "\n",
        "# Step 6: Apply PCA (keep top 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Step 7: Train KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"Accuracy on PCA (2 components) data:\", round(acc_pca, 3))\n",
        "\n",
        "# Step 8: Compare Results\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Original Data Accuracy : {acc_original:.3f}\")\n",
        "print(f\"PCA (2 Components) Accuracy : {acc_pca:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5exv_W8pwVh",
        "outputId": "8844949a-b1e7-4c7f-dfad-d5fdd8654dea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original scaled data: 0.944\n",
            "Accuracy on PCA (2 components) data: 0.944\n",
            "\n",
            "Comparison:\n",
            "Original Data Accuracy : 0.944\n",
            "PCA (2 Components) Accuracy : 0.944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.**"
      ],
      "metadata": {
        "id": "3AfJPuKQqAe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Wine Dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Step 3: Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 4: Scale Features (important for distance-based algorithms)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train KNN with Euclidean Distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Step 6: Train KNN with Manhattan Distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Step 7: Compare Results\n",
        "print(\"Accuracy using Euclidean distance :\", round(acc_euclidean, 3))\n",
        "print(\"Accuracy using Manhattan distance :\", round(acc_manhattan, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfrWe-PwqJ6a",
        "outputId": "efa0ea1f-b7b1-42b9-e409-41fd20f9fa5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Euclidean distance : 0.944\n",
            "Accuracy using Manhattan distance : 0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data**"
      ],
      "metadata": {
        "id": "iHpbfPs5qOYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-dimensional gene expression datasets often have thousands of features (genes) but few samples (patients). This creates a high risk of overfitting for traditional models because they try to fit noise instead of true patterns. Using PCA + KNN is a common and effective pipeline to handle this.\n",
        "\n",
        "**Step 1: Use PCA to Reduce Dimensionality**\n",
        "\n",
        "Why: Thousands of genes are often highly correlated, and many may contain noise or redundant information. PCA reduces dimensionality while retaining the most important variance.\n",
        "\n",
        "**How:**\n",
        "\n",
        "Standardize the gene expression data (mean=0, std=1).\n",
        "\n",
        "Apply PCA to transform the data into principal components — new axes that summarize variance.\n",
        "\n",
        "This step compresses thousands of features into fewer components, making downstream models like KNN feasible.\n",
        "\n",
        "**Step 2: Decide How Many Components to Keep**\n",
        "\n",
        "Use Explained Variance Ratio:\n",
        "\n",
        "Compute how much variance each principal component explains.\n",
        "\n",
        "Retain the top components that together explain ~80–95% of total variance.\n",
        "\n",
        "**Example:** If 100 components explain 90% variance, you can reduce 10,000 genes to just 100 components.\n",
        "\n",
        "**Optional:** Scree plot — visualize cumulative variance and choose the “elbow” point where adding more components adds little extra information.\n",
        "\n",
        "**Step 3: Use KNN for Classification Post-Dimensionality Reduction**\n",
        "\n",
        "**Why KNN:**\n",
        "\n",
        "KNN is a simple, non-parametric, distance-based algorithm — suitable for small sample sizes after dimensionality reduction.\n",
        "\n",
        "**How:**\n",
        "\n",
        "Train KNN on PCA-transformed components.\n",
        "\n",
        "Choose K carefully using cross-validation.\n",
        "\n",
        "Distance metric: usually Euclidean, but Manhattan can be tested for robustness.\n",
        "\n",
        "**Step 4: Evaluate the Model**\n",
        "\n",
        "Metrics: Accuracy, precision, recall, F1-score — depending on clinical relevance.\n",
        "\n",
        "Cross-Validation: Use k-fold cross-validation or leave-one-out to ensure the model is robust and not overfitting.\n",
        "\n",
        "Optional: ROC-AUC for multi-class evaluation.\n",
        "\n",
        "**Step 5: Justify the Pipeline to Stakeholders**\n",
        "\n",
        "Problem: High-dimensional gene data → overfitting in traditional models.\n",
        "\n",
        "Solution:\n",
        "\n",
        "PCA reduces noise and compresses features → reduces overfitting risk.\n",
        "\n",
        "KNN classifies patients based on true biological similarity (distance in compressed space).\n",
        "\n",
        "Cross-validation ensures the model generalizes to new patients.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "Simple, interpretable, and computationally feasible.\n",
        "\n",
        "Preserves the most biologically meaningful variance.\n",
        "\n",
        "Avoids overfitting due to small sample size.\n",
        "\n",
        "Can be visualized (e.g., first 2–3 PCA components) for presentation to clinicians.\n",
        "\n",
        "**Summary Statement**\n",
        "\n",
        "By combining PCA with KNN, we create a robust, interpretable, and scalable pipeline for high-dimensional biomedical data. PCA reduces dimensionality and noise, KNN leverages the true structure of patient similarity for classification, and rigorous cross-validation ensures reliable predictions — making it suitable for real-world clinical applications."
      ],
      "metadata": {
        "id": "B1R458aIqZ6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PzXrJjvsrcDG"
      }
    }
  ]
}